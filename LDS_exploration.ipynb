{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ace184-f9d3-4a60-ba5d-3fa378754b31",
   "metadata": {},
   "source": [
    "# Exploration 1\n",
    "\n",
    "What if we start small (let $\\ell = m = n = 3$ so that we are tracking three variables with three neurons) and attempt to train the neural net to learn the Kalman filter? What does the loss look like over time for various initial values of $M$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7bc73a-334d-4838-a9f4-1c8663b8e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "import tqdm\n",
    "\n",
    "from hm_process import HMProcess, plot_hm_process\n",
    "from kalman import SteadyStateKalmanFilter\n",
    "from rnn import NeuralNet\n",
    "from utils import calc_loss, mk_rand_matrix, mk_rand_matrix_envals\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56928f67-198f-4b60-b0b2-6ffa18f388c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some arbitrary parameters\n",
    "def create_process_3d(rng, A=None):\n",
    "    if A is None:\n",
    "        A = np.diag([0.98, 0.96, 0.94])\n",
    "    x0 = np.array([100, 100, 100])\n",
    "    sigma_process = 10\n",
    "    Sigma_process = (sigma_process**2)*np.eye(3)\n",
    "    O = np.eye(3)\n",
    "    sigma_obs = 20\n",
    "    Sigma_obs = (sigma_obs**2)*np.eye(3)\n",
    "    num_steps = 100\n",
    "    return HMProcess(rng, A, x0, Sigma_process, O, Sigma_obs, num_steps)\n",
    "\n",
    "proc = create_process_3d(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2746824-070d-4f68-90d6-7412743fa214",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts, xs, ys = proc.simulate()\n",
    "plot_hm_process('A hidden Markov process governed by linear dynamics', ts, xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362488d0-fe4e-429f-88bc-d665e5b3d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, what if we try to predict what the latent states were based\n",
    "# on the observations by using a steady-state Kalman filter, assuming\n",
    "# we know the true values of A, Sigma_process, O, and Sigma_obs?\n",
    "ts, xs, ys = proc.simulate()\n",
    "kf = SteadyStateKalmanFilter(proc)\n",
    "xhats = kf.infer(ys)\n",
    "\n",
    "# Now, let's plot it!\n",
    "plot_hm_process('A hidden Markov process governed by linear dynamics', ts, xs, ys)\n",
    "plot_hm_process('Steady-state Kalman filter predictions', ts, xs, ys, xhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7143d47f-8560-408f-abe3-883f6c37c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how well the Kalman filter converges to the steady-state Kalman filter.\n",
    "Sigma_infty_dist, K_infty_dist, M_infty_dist, _, _, _ = kf.check_convergence()\n",
    "plt.plot(np.arange(proc.num_steps), K_infty_dist/np.linalg.norm(kf.K_infty, ord='fro'), label='Kalman gain')\n",
    "plt.plot(np.arange(proc.num_steps), M_infty_dist/np.linalg.norm(kf.M_infty, ord='fro'), label='Transition')\n",
    "plt.plot(np.arange(proc.num_steps), Sigma_infty_dist/np.linalg.norm(kf.Sigma_infty, ord='fro'), label='Prediction covariance')\n",
    "plt.title('Convergence of Kalman filter parameters')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frobenius distance divided by norm of steady state')\n",
    "plt.xlim(0,10)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac10bbf6-f1cb-46e2-87bf-cad3c2373721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's see if we can train a linear dynamical system to learn the Kalman filter.\n",
    "# First, let's make the matrix A of the hidden Markov process random.\n",
    "A = mk_rand_matrix_envals(rng, np.array([0.98, 0.96, 0.94]))\n",
    "proc = create_process_3d(rng, A)\n",
    "kf = SteadyStateKalmanFilter(proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eae8413-ef7f-40d4-8a3d-f144ffa6282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How well does the network do if I make the connectivity matrix\n",
    "# closer or farther from the Kalman filter theory steady-state value?\n",
    "def test_M_landscape():\n",
    "    distances = []\n",
    "    losses = []\n",
    "    for _ in range(3):\n",
    "        M = mk_rand_matrix(rng, 3)\n",
    "        for i in tqdm.tqdm(range(10)):\n",
    "            c = i/10\n",
    "            M2 = (1-c)*M + c*kf.M_infty\n",
    "            nn = NeuralNet(kf.K_infty, M2, np.eye(3), proc.x0)\n",
    "            Ls = nn.batch_test(100, proc)\n",
    "            distances.append(np.linalg.norm(M2 - kf.M_infty, ord='fro'))\n",
    "            losses.append(np.mean(Ls))\n",
    "    return np.array(distances), np.array(losses)\n",
    "\n",
    "#distances, losses = test_M_landscape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d43727-3d8d-4ec7-8dd9-323c9779f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try plotting our results\n",
    "#plt.scatter(distances[:10], losses[:10])\n",
    "#plt.scatter(distances[10:20], losses[10:20])\n",
    "#plt.scatter(distances[20:], losses[20:])\n",
    "#plt.xlabel('Frobenius distance between M and the optimal value')\n",
    "#plt.ylabel('Average loss (100 simulated trials)')\n",
    "#plt.title('Loss vs distance from Kalman filter theory')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381c73b-cdac-4749-9624-070c6a7ca5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens when we train the neural net on this task?\n",
    "def compare_nn_kalman():\n",
    "    nn = NeuralNet(0.1*mk_rand_matrix(rng, 3), 0.1*mk_rand_matrix(rng, 3), np.eye(3), proc.x0)\n",
    "    ts, xs, ys = proc.simulate()\n",
    "    \n",
    "    _, xhats = nn.forward(ys)\n",
    "    plot_hm_process('Before training', ts, xs, ys, xhats)\n",
    "    print('Loss', calc_loss(xhats, xs))\n",
    "    print('M =', nn.M)\n",
    "    print('K =', nn.K)\n",
    "    \n",
    "    losses = nn.train(np.array([1e-6]*10), 40, proc)\n",
    "    plt.plot(losses)\n",
    "    plt.title('Loss over time')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Mean loss')\n",
    "    plt.show()\n",
    "    \n",
    "    _, xhats = nn.forward(ys)\n",
    "    plot_hm_process('After training', ts, xs, ys, xhats)\n",
    "    print('Loss', calc_loss(xhats, xs))\n",
    "    print('M =', nn.M)\n",
    "    print('K =', nn.K)\n",
    "    \n",
    "    # How does it compare to a Kalman filter?\n",
    "    xhats = kf.infer(ys)\n",
    "    plot_hm_process('Kalman filter', ts, xs, ys, xhats)\n",
    "    print('Loss', calc_loss(xhats, xs))\n",
    "    print('M =', kf.M_infty)\n",
    "    print('K =', kf.K_infty)\n",
    "\n",
    "#compare_nn_kalman()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b057b5-66dc-4d91-8693-b2d7aa98696c",
   "metadata": {},
   "source": [
    "# Exploration 2\n",
    "\n",
    "What's the right measure of distance between $(M,K)$ and $A$ that corresponds to the average loss at the task? We'll assume $O$ and $W$ are the identity. Can we plot a loss landscape for the task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca2bc7-751b-4a41-aa28-cf9d277fc1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with 1 dimension for easy visualization\n",
    "def create_1d_process(rng, A):\n",
    "    x0 = np.array([100])\n",
    "    sigma_process = 10\n",
    "    Sigma_process = (sigma_process**2)*np.eye(1)\n",
    "    O = np.array([[1]])\n",
    "    sigma_obs = 20\n",
    "    Sigma_obs = (sigma_obs**2)*np.eye(1)\n",
    "    num_steps = 50\n",
    "    return HMProcess(rng, A, x0, Sigma_process, O, Sigma_obs, num_steps)\n",
    "\n",
    "batch_size = 5\n",
    "num_a_ticks = 5\n",
    "num_ticks = 20\n",
    "a_vals = np.array([-0.95, -0.8, 0, 0.8, 0.95])\n",
    "num_a_ticks = a_vals.shape[0]\n",
    "mk_range = 0.99\n",
    "num_trials_per = 40\n",
    "W = np.array([[1]])\n",
    "start_from = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2546b6f8-b139-49ce-8267-3b01c427df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nn_values_A_M_K_1d(num_nns=2):\n",
    "    mks = [] # mks[i][j] contains the values of M and K for the jth trial with the ith value of A\n",
    "\n",
    "    for i, a_ in enumerate(a_vals):\n",
    "        A = np.array([[a_]])\n",
    "        mks_i = []\n",
    "        for j in tqdm.tqdm(range(num_nns)):\n",
    "            proc = create_1d_process(rng, A)\n",
    "            nn = NeuralNet(0.1*mk_rand_matrix(rng, 1), 0.1*mk_rand_matrix(rng, 1), np.eye(1), proc.x0)\n",
    "            _, _, Ms, Ks = nn.train_until_converge(5e-7, 200, num_trials_per, proc, start_from,\n",
    "                                                   print_loss=False, progress_bar=False)\n",
    "            mks_i.append((Ms.flatten(), Ks.flatten()))\n",
    "        mks.append(mks_i)\n",
    "\n",
    "    return mks\n",
    "\n",
    "#mks = find_nn_values_A_M_K_1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da403dac-65b5-4286-a173-aeb98e56c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_A_M_K_loss_landscape_1d():\n",
    "    losses = np.zeros((num_a_ticks,num_ticks,num_ticks))\n",
    "    W = np.array([[1]])\n",
    "    \n",
    "    for i1, a_ in tqdm.tqdm(enumerate(a_vals), total=num_a_ticks):\n",
    "        for i2, m_ in enumerate(np.linspace(-1*mk_range,mk_range,num_ticks)):\n",
    "            for i3, k_ in enumerate(np.linspace(-1*mk_range,mk_range,num_ticks)):\n",
    "                A = np.array([[a_]])\n",
    "                M = np.array([[m_]])\n",
    "                K = np.array([[k_]])\n",
    "                proc = create_1d_process(rng, A)\n",
    "                nn = NeuralNet(M, K, W, proc.x0)\n",
    "                Ls = nn.batch_test(batch_size, proc, start_from)\n",
    "                losses[i1,i2,i3] = np.mean(Ls)\n",
    "    \n",
    "    return losses\n",
    "\n",
    "#losses = test_A_M_K_loss_landscape_1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0675a0d-2859-4864-8468-4d578375f4a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#norm = matplotlib.colors.Normalize(3, 4.8)\n",
    "#for i1, a_ in enumerate(a_vals):\n",
    "#\n",
    "#    # Loss landscape\n",
    "#    c = plt.imshow(np.log10(losses[i1].T), origin='lower', extent=(-1*mk_range,mk_range,-1*mk_range,mk_range), norm=norm)\n",
    "#    plt.xlabel('M')\n",
    "#    plt.ylabel('K')\n",
    "#\n",
    "#    # Where does the Kalman filter fall?\n",
    "#    A = np.array([[a_]])\n",
    "#    proc = create_1d_process(rng, A)\n",
    "#    kf = SteadyStateKalmanFilter(proc)\n",
    "#    plt.plot(kf.M_infty, kf.K_infty, color='black', marker='o')\n",
    "#    plt.annotate('Kalman filter', (kf.M_infty, kf.K_infty),\n",
    "#                 xytext=(kf.M_infty-0.2, kf.K_infty+0.05), color='black')\n",
    "#\n",
    "#    # What about the RNNs?\n",
    "#    colors = ['red', 'tomato']\n",
    "#    for j in range(2):\n",
    "#        plt.plot(mks[i1][j][0], mks[i1][j][1], color=colors[j], linestyle='-', marker='o', markersize=3)\n",
    "#    \n",
    "#    plt.title(f'(M, K) Log-Loss Landscape For A={a_:.2f}')\n",
    "#    plt.xlim(-1*mk_range, mk_range)\n",
    "#    plt.ylim(-1*mk_range, mk_range)\n",
    "#    plt.colorbar(c)\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4916e-9dac-4541-ae90-95c96e40c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def test_mk(proc, m_, k_):\n",
    "#    A, M, K = proc.A, np.array([[m_]]), np.array([[k_]])\n",
    "#    nn = NeuralNet(M, K, W, proc.x0)\n",
    "#    proc = create_1d_process(rng, A)\n",
    "#    losses = nn.batch_test(100, proc)\n",
    "#    print(np.mean(losses))\n",
    "\n",
    "#A = np.array([[a_vals[-1]]])\n",
    "#proc = create_1d_process(rng, A)\n",
    "#kf = SteadyStateKalmanFilter(proc)\n",
    "\n",
    "#m_ = mks[-1][0][0][-1]\n",
    "#k_ = mks[-1][0][1][-1]\n",
    "#test_mk(proc, m_, k_)\n",
    "#test_mk(proc, kf.M_infty, kf.K_infty)\n",
    "\n",
    "#ts, xs, ys = proc.simulate()\n",
    "\n",
    "#nn = NeuralNet(np.array([[m_]]), np.array([[k_]]), np.eye(1), proc.x0)\n",
    "#_, xhats = nn.forward(ys)\n",
    "#plot_hm_process('After training', ts, xs, ys, xhats)\n",
    "#print('Loss', calc_loss(xhats, xs, start_from))\n",
    "#print('M =', nn.M)\n",
    "#print('K =', nn.K)\n",
    "\n",
    "#xhats = kf.infer(ys)\n",
    "#plot_hm_process('Kalman filter', ts, xs, ys, xhats)\n",
    "#print('Loss', calc_loss(xhats, xs, start_from))\n",
    "#print('M =', kf.M_infty)\n",
    "#print('K =', kf.K_infty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72807c2-151d-47b9-8755-7329140c0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about two dimensions?\n",
    "def create_2d_process(rng, A):\n",
    "    x0 = np.array([100, 100])\n",
    "    sigma_process = 5\n",
    "    Sigma_process = (sigma_process**2)*np.eye(2)\n",
    "    O = np.eye(2)\n",
    "    sigma_obs = 20\n",
    "    Sigma_obs = (sigma_obs**2)*np.eye(2)\n",
    "    num_steps = 50\n",
    "    return HMProcess(rng, A, x0, Sigma_process, O, Sigma_obs, num_steps)\n",
    "\n",
    "batch_size = 5\n",
    "num_a_ticks = 5\n",
    "num_ticks = 20\n",
    "A = mk_rand_matrix_envals(rng, np.array([0.9, 0.97]))\n",
    "proc = create_2d_process(rng, A)\n",
    "kf = SteadyStateKalmanFilter(proc)\n",
    "mk_range = 0.99\n",
    "num_trials_per = 40\n",
    "W = np.eye(2)\n",
    "start_from = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc007a0-413c-443c-a665-d2b79f6a127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_K_loss_landscape_2d():\n",
    "    M = kf.M_infty\n",
    "    \n",
    "    losses = np.zeros((num_ticks,num_ticks))    \n",
    "    for i_k1, k_ev1 in tqdm.tqdm(enumerate(np.linspace(-1*mk_range,mk_range,num_ticks)), total=num_ticks):\n",
    "        for i_k2, k_ev2 in enumerate(np.linspace(-1*mk_range,mk_range,num_ticks)):\n",
    "            K = mk_rand_matrix_envals(rng, np.array([k_ev1, k_ev2]))\n",
    "            nn = NeuralNet(M, K, W, proc.x0)\n",
    "            Ls = nn.batch_test(batch_size, proc, start_from)\n",
    "            losses[i_k1,i_k2] = np.mean(Ls)\n",
    "    \n",
    "    return losses\n",
    "\n",
    "#losses_K = test_K_loss_landscape_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceae4351-fd29-4c73-8a30-171c4bd82ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_M_loss_landscape_2d():\n",
    "    K = kf.K_infty\n",
    "    \n",
    "    losses = np.zeros((num_ticks,num_ticks))    \n",
    "    for i_m1, m_ev1 in tqdm.tqdm(enumerate(np.linspace(-1*mk_range,mk_range,num_ticks)), total=num_ticks):\n",
    "        for i_m2, m_ev2 in enumerate(np.linspace(-1*mk_range,mk_range,num_ticks)):\n",
    "            M = mk_rand_matrix_envals(rng, 2, np.array([m_ev1, m_ev2]))\n",
    "            nn = NeuralNet(M, K, W, proc.x0)\n",
    "            Ls = nn.batch_test(batch_size, proc, start_from)\n",
    "            losses[i_m1,i_m2] = np.mean(Ls)\n",
    "    \n",
    "    return losses\n",
    "\n",
    "#losses_M = test_M_loss_landscape_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a638f-6ea5-4235-a23e-2727e8ba73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_M_loss_landscape_2d_diagentry():\n",
    "    K = kf.K_infty\n",
    "    M = kf.K_infty\n",
    "    \n",
    "    losses = np.zeros((num_ticks,num_ticks))    \n",
    "    for i_m1, m1 in tqdm.tqdm(enumerate(np.linspace(-1*mk_range,mk_range,num_ticks)), total=num_ticks):\n",
    "        for i_m2, m2 in enumerate(np.linspace(-1*mk_range,mk_range,num_ticks)):\n",
    "            M[0,0] = m1\n",
    "            M[1,1] = m2\n",
    "            nn = NeuralNet(M, K, W, proc.x0)\n",
    "            Ls = nn.batch_test(batch_size, proc, start_from)\n",
    "            losses[i_m1,i_m2] = np.mean(Ls)\n",
    "    \n",
    "    return losses\n",
    "\n",
    "#losses_M_diag = test_M_loss_landscape_2d_diagentry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b810f8af-2d40-41e2-81d0-4993ba4c3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss landscape\n",
    "#norm = matplotlib.colors.Normalize(4, 6)\n",
    "#c = plt.imshow(np.log10(losses_M_diag.T), origin='lower', extent=(-1*mk_range,mk_range,-1*mk_range,mk_range), norm=norm)\n",
    "#plt.xlabel('M diag 1')\n",
    "#plt.ylabel('M diag 2')\n",
    "\n",
    "# Where does the Kalman filter fall?\n",
    "#envals = (kf.M_infty[0,0], kf.M_infty[1,1])\n",
    "#plt.plot(envals[0], envals[1], color='black', marker='o')\n",
    "#plt.annotate('Kalman filter', (envals[0], envals[1]),\n",
    "#             xytext=(envals[0]-0.2, envals[1]+0.05), color='black')\n",
    "\n",
    "# What about the RNNs?\n",
    "#plt.scatter(mks[i1,:,0], mks[i1,:,1], color='red', marker='o')\n",
    "\n",
    "#plt.title(f'M Log-Loss Landscape')\n",
    "#plt.xlim(-1*mk_range, mk_range)\n",
    "#plt.ylim(-1*mk_range, mk_range)\n",
    "#plt.colorbar(c)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5e1dc-46cf-460f-b641-4e3598e675f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kf = SteadyStateKalmanFilter(proc)\n",
    "#ts, xs, ys = proc.simulate()\n",
    "\n",
    "#nn = NeuralNet(np.diag((0,0.5)), kf.K_infty, np.eye(2), proc.x0)\n",
    "#_, xhats = nn.forward(ys)\n",
    "#plot_hm_process('After training', ts, xs, ys, xhats)\n",
    "#print('Loss', calc_loss(xhats, xs, start_from))\n",
    "#print('M =', nn.M)\n",
    "#print('K =', nn.K)\n",
    "\n",
    "#xhats = kf.infer(ys)\n",
    "#plot_hm_process('Kalman filter', ts, xs, ys, xhats)\n",
    "#print('Loss', calc_loss(xhats, xs, start_from))\n",
    "#print('M =', kf.M_infty)\n",
    "#print('K =', kf.K_infty)\n",
    "\n",
    "#print(kf.Sigma_infty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cad2e5-b296-4208-b0b5-bc46cd6453ad",
   "metadata": {},
   "source": [
    "# Exploration 3\n",
    "The goal of this exploration is to work out some possible bugs with the implementation of the Kalman filter (why does the covariance matrix not converge properly?) and find an appropriate method to systematically determine values of the buy-in period (at what time step do we start the loss calculation?) and input gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c249c-2a7c-49f5-ab10-0b76b7837ea7",
   "metadata": {},
   "source": [
    "First bug solved: the issue was that I was plotting the distance between the posterior estimate covariance and the steady-state estimate covariance when I should have been plotting the distance between the prior estimate covariance and the steady-state estimate covariance instead. Now the graph looks as expected.\n",
    "\n",
    "Let's experiment next with simulating 1D processes and checking how long the buy-in period is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7826bd31-efc3-4dcb-aae9-654e0303f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_process_1d_more_parameters(rng, a, sigma_process, sigma_obs):\n",
    "    A = np.array([[a]])\n",
    "    x0 = np.array([100])\n",
    "    Sigma_process = (sigma_process**2)*np.eye(1)\n",
    "    O = np.eye(1)\n",
    "    Sigma_obs = (sigma_obs**2)*np.eye(1)\n",
    "    num_steps = 50\n",
    "    return HMProcess(rng, A, x0, Sigma_process, O, Sigma_obs, num_steps)\n",
    "\n",
    "def time_of_convergence(proc, eps=0.05, i=0):\n",
    "    kf = SteadyStateKalmanFilter(proc)\n",
    "    Sigma_infty_dist, K_infty_dist, M_infty_dist, _, _, _ = kf.check_convergence()\n",
    "    for i in range(proc.num_steps-1,-1,-1):\n",
    "        if Sigma_infty_dist[i] > eps:\n",
    "            return i+1\n",
    "        if K_infty_dist[i] > eps:\n",
    "            return i+1\n",
    "        if M_infty_dist[i] > eps:\n",
    "            return i+1\n",
    "    return 0\n",
    "\n",
    "def plot_1d_kalman_convergence_dynamics():\n",
    "    num_points = 100\n",
    "    a_vals = np.linspace(0.01,0.99,num_points)\n",
    "    t_vals = np.zeros(num_points)\n",
    "    for i in range(num_points):\n",
    "        proc = create_process_1d_more_parameters(rng, a_vals[i], 10, 25)\n",
    "        t_vals[i] = time_of_convergence(proc)\n",
    "    plt.plot(a_vals, t_vals, label='Actual trend')\n",
    "    plt.plot(a_vals, -1/np.log(a_vals), label='Proportional to time constant')\n",
    "    plt.plot(a_vals, 11*a_vals+2, label='Roughly linear')\n",
    "    plt.title('Length of buy-in period vs largest eigenvalue (1D)')\n",
    "    plt.xlabel('Largest eigenvalue of A')\n",
    "    plt.ylabel('Time until <5% error')\n",
    "    plt.ylim(0,12)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_1d_kalman_convergence_dynamics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8fdeef-3e61-458b-bf23-fd366f8ae0c9",
   "metadata": {},
   "source": [
    "It looks like the fit is not proportional to the time constant of the process, as I would have expected, but instead is roughly linear! Choosing a buy-in time period of eleven times the value of A plus two should be sufficient for letting the Kalman filter converge. What if we try changing the noise parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70adf891-924d-4cc0-8b1f-b79679622c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1d_kalman_convergence_noise():\n",
    "    num_points = 30\n",
    "    max_proc_noise = 50\n",
    "    max_obs_noise = 50\n",
    "    sigma_proc_vals = np.linspace(1,max_proc_noise,num_points)\n",
    "    sigma_obs_vals = np.linspace(1,max_obs_noise,num_points)\n",
    "    t_vals = np.zeros((num_points,num_points))\n",
    "    for i in range(num_points):\n",
    "        for j in range(num_points):\n",
    "            proc = create_process_1d_more_parameters(rng, 0.99, sigma_proc_vals[i], sigma_obs_vals[j])\n",
    "            t_vals[i,j] = time_of_convergence(proc)\n",
    "    norm = matplotlib.colors.Normalize(0, 11)\n",
    "    plt.imshow(t_vals, extent=(1,max_obs_noise,1,max_proc_noise), origin='lower', norm=norm)\n",
    "    x_vals = np.linspace(1, max_obs_noise, 200)\n",
    "    plt.plot(x_vals, x_vals/2, color='red')\n",
    "    plt.title('Length of buy-in period vs noise (1D)')\n",
    "    plt.xlabel('Observation noise standard deviation')\n",
    "    plt.ylabel('Process noise standard deviation')\n",
    "    plt.xlim(1, max_obs_noise)\n",
    "    plt.ylim(1, max_proc_noise)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "plot_1d_kalman_convergence_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8de00-e43d-49d3-9ef4-f995f0df9c59",
   "metadata": {},
   "source": [
    "Process and observation noise seem to make the convergence take much longer when the observation noise is high and the process noise is very low. A=0.99 originally took 11 steps, and most of this graph is in the non-yellow region where the number of steps needed is below 11 steps, as long as the observation noise is less than twice the process noise (which is the red line).\n",
    "\n",
    "Now, let's try it with multi-dimensional processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a7a9a-901f-465c-882a-2682f09087e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_process_3d_based_on_envals(rng, enval1, enval2, enval3):\n",
    "    A = mk_rand_matrix_envals(rng, np.array([enval1, enval2, enval3]))\n",
    "    x0 = np.array([100, 100, 100])\n",
    "    sigma_process = 10\n",
    "    Sigma_process = (sigma_process**2)*np.eye(3)\n",
    "    O = np.eye(3)\n",
    "    sigma_obs = 25\n",
    "    Sigma_obs = (sigma_obs**2)*np.eye(3)\n",
    "    num_steps = 50\n",
    "    return HMProcess(rng, A, x0, Sigma_process, O, Sigma_obs, num_steps)\n",
    "\n",
    "def plot_3d_kalman_convergence_dynamics():\n",
    "    max_envals = []\n",
    "    t_vals = []\n",
    "    num_points = 10\n",
    "    enval_space = np.linspace(0.01,0.99,num_points)\n",
    "    for enval1 in enval_space:\n",
    "        for enval2 in enval_space:\n",
    "            for enval3 in enval_space:\n",
    "                try:\n",
    "                    proc = create_process_3d_based_on_envals(rng, enval1, enval2, enval3)\n",
    "                    t_vals.append(time_of_convergence(proc))\n",
    "                    max_envals.append(max(enval1, enval2, enval3))\n",
    "                except sklearn.exceptions.ConvergenceWarning:\n",
    "                    pass\n",
    "    \n",
    "    plt.scatter(max_envals, t_vals)\n",
    "    plt.title('Length of buy-in period vs largest eigenvalue (3D)')\n",
    "    plt.xlabel('Largest eigenvalue of A')\n",
    "    plt.ylabel('Time until <5% error')\n",
    "    plt.show()\n",
    "\n",
    "plot_3d_kalman_convergence_dynamics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0501bc-ca40-4793-9c84-596bb80d8a45",
   "metadata": {},
   "source": [
    "The graph has roughly the same shape as in the 1D case, which is reassuring, although there are some matrices which are outliers. Let's check one of them by hand. All in all, I conclude that if the process noise is more than half the observation noise, then a buy-in period of a little bit more than 10 times the largest eigenvalue of the dynamics matrix should suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fcf08d-57c1-49d1-99ed-db4a2ea74665",
   "metadata": {},
   "source": [
    "Next, how do we deal with the input gain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699829d-76d9-41e9-b674-8add0dc421be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_appropriate_gain(proc, M):\n",
    "    # return best K?\n",
    "    pass\n",
    "\n",
    "def plot_gain_function():\n",
    "    norm = matplotlib.colors.Normalize(3, 4.8)\n",
    "    for i1, a_ in enumerate(a_vals):\n",
    "    \n",
    "        # Loss landscape\n",
    "        c = plt.imshow(np.log10(losses[i1].T), origin='lower', extent=(-1*mk_range,mk_range,-1*mk_range,mk_range), norm=norm)\n",
    "        plt.xlabel('M')\n",
    "        plt.ylabel('K')\n",
    "    \n",
    "        # Best values for K, given M?\n",
    "        ms = np.linspace(-1*mk_range, mk_range, 100)\n",
    "        ks = []\n",
    "        proc = create_1d_process(rng, np.array([[a_]]))\n",
    "        for m_ in ms:\n",
    "            K = find_appropriate_gain(proc, np.array([[m_]]))\n",
    "            ks.append(K[0,0])\n",
    "        ks = np.array(ks)\n",
    "        plt.plot(ms, ks, color='red')\n",
    "        \n",
    "        plt.title(f'(M, K) Log-Loss Landscape For A={a_:.2f}')\n",
    "        plt.xlim(-1*mk_range, mk_range)\n",
    "        plt.ylim(-1*mk_range, mk_range)\n",
    "        plt.colorbar(c)\n",
    "        plt.show()\n",
    "\n",
    "#plot_gain_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f4d8e8-bcee-4fe1-b767-33df35afe6c9",
   "metadata": {},
   "source": [
    "# Exploration 4\n",
    "Now, what if we add some structure onto our connectivity matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552f60f-baa0-4683-88e5-ce5ab9d6241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if the loss is impacted by random masks\n",
    "\n",
    "def mk_rand_mask(num_neurons=3):\n",
    "    return np.random.randint(0,2,size=(num_neurons,num_neurons))\n",
    "\n",
    "def test_masks():\n",
    "    for _ in range(2):\n",
    "        mask = mk_rand_mask()\n",
    "        print(mask)\n",
    "        nn = NeuralNet(0.1*mk_rand_matrix(rng, 3), 0.1*mk_rand_matrix(rng, 3), np.eye(3), proc.x0, mask=mask)\n",
    "        num_batches, losses = nn.train_until_converge(1e-6, 1000, 20, proc)\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel('Batch')\n",
    "        plt.ylabel('Mean loss')\n",
    "        plt.show()\n",
    "\n",
    "#test_masks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
